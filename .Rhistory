library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster() # stop the cluster if you don't need it anymore
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
folder = "/Users/haonguyen/Desktop/CIS8393 - Using R/aclImdb/test"
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster()
folder = "/Users/haonguyen/Desktop/CIS8393 - Using R/aclImdb/"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
install.packages(c("foreach", "doParallel"))
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster() #
install.packages(c("foreach", "doParallel"))
setwd("/Users/haonguyen/Desktop/CIS8393 - Using R/")
folder = "aclImdb/"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster()
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
library(tidytext)
library(stringr)
library(tidyverse)
setwd("/Users/haonguyen/Desktop/CIS8393 - Using R/")
folder = "aclImdb/"
fnames = list.files(folder, recursive = T) #get all filenames under aclImdb
#we are only interested in the txt files under the neg/pos folder
fnames=fnames[str_detect(fnames, "(neg|pos)/.+txt")]
ll = length(fnames)
ll
walk(fnames[1:6], print)
walk(fnames[(ll-5):ll], print)
#install.packages(c("foreach", "doParallel"))
library(purrr)
library(foreach)
library(doParallel)
n_core = parallel::detectCores()
registerDoParallel(n_core) #initiate a parallel cluster
# read files into R in parallel
txts = foreach(i = 1:length(fnames),
.combine = c, .packages = "tidyverse") %dopar% {
read_file(str_c(folder, fnames[i]))
}
stopImplicitCluster() # stop the cluster if you don't need it anymore
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df = data_frame(fname = fnames, text = txts)
df
df = df %>%
separate(fname,
into=c("type", "polarity", "review_id", "rating", "ext"),
sep="/|_|\\.") %>%
arrange(review_id)
df
df = df %>%
mutate(doc_id = str_c(type, polarity, review_id, sep = "_")) %>%
mutate(text = str_replace_all(text, "(<br />)+", " "),
review_id = as.numeric(review_id),
rating = as.numeric(rating)) %>%
select(type, polarity, rating, review_id, doc_id, text)
df
library(tidytext)
tokens <- df %>%
unnest_tokens(output = word, input = text)
tokens
tokens %>%
count(word,
sort = TRUE)
sw = get_stopwords()
sw
cleaned_tokens <- tokens %>%
filter(!word %in% sw$word)
nums <- cleaned_tokens %>%
filter(str_detect(word, "^[0-9]")) %>%
select(word) %>% unique()
nums
cleaned_tokens <- cleaned_tokens %>%
filter(!word %in% nums$word)
length(unique(cleaned_tokens$word))
cleaned_tokens %>%
count(word, sort = T) %>%
rename(word_freq = n) %>%
ggplot(aes(x=word_freq)) +
geom_histogram(aes(y=..count..), color="black", fill="blue", alpha=0.3) +
scale_x_continuous(breaks=c(0:5,10,100,500,10e3),
trans="log1p", expand=c(0,0)) +
scale_y_continuous(breaks=c(0,100,1000,5e3,10e3,5e4,10e4,4e4), expand=c(0,0)) +
theme_bw()
rare <- cleaned_tokens %>%
count(word) %>%
filter(n<10) %>%
select(word) %>%
unique()
rare
cleaned_tokens <- cleaned_tokens %>%
filter(!word %in% rare$word)
length(unique(cleaned_tokens$word)
)
library(wordcloud)
# define a nice color palette
pal <- brewer.pal(8,"Dark2")
# plot the 100 most common words
cleaned_tokens %>%
count(word) %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
get_sentiments("nrc")
get_sentiments("afinn")
library(tidytext)
get_sentiments("nrc")
get_sentiments("afinn")
library(textdata)
install.packages("textdata")
library(textdata)
get_sentiments("nrc")
sent_reviews = cleaned_tokens %>%   left_join(get_sentiments("nrc")) %>%  rename(nrc = sentiment) %>%  left_join(get_sentiments("bing")) %>%  rename(bing = sentiment) %>%  left_join(get_sentiments("afinn")) %>%  rename(afinn = value)sent_reviews
sent_reviews = cleaned_tokens %>%   left_join(get_sentiments("nrc")) %>%  rename(nrc = sentiment) %>%  left_join(get_sentiments("bing")) %>%  rename(bing = sentiment) %>%  left_join(get_sentiments("afinn")) %>%  rename(afinn = value)sent_reviews
sent_reviews = cleaned_tokens %>%
left_join(get_sentiments("nrc")) %>%
rename(nrc = sentiment) %>%
left_join(get_sentiments("bing")) %>%
rename(bing = sentiment) %>%
left_join(get_sentiments("afinn")) %>%
rename(afinn = value)
sent_reviews = cleaned_tokens %>%
left_join(get_sentiments("nrc")) %>%
rename(nrc = sentiment) %>%
left_join(get_sentiments("bing")) %>%
rename(bing = sentiment) %>%
left_join(get_sentiments("afinn")) %>%
rename(afinn = value)
sent_reviews
bing_word_counts <- sent_reviews %>%
filter(!is.na(bing)) %>%
count(word, bing, sort = TRUE)
bing_word_counts
bing_word_counts %>%
filter(n > 10000) %>%
mutate(n = ifelse(bing == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = bing)) +  geom_col() +  coord_flip() +  labs(y = "Contribution to sentiment")
ncr_word_counts <- sent_reviews %>%
filter(!is.na(ncr)) %>%
count(word, ncr, sort = TRUE)
ncr_word_counts
nrc_word_counts <- sent_reviews %>%
filter(!is.na(nrc)) %>%
count(word, ncr, sort = TRUE)
nrc_word_counts
nrc_word_counts <- sent_reviews %>%
filter(!is.na(nrc)) %>%
count(word, nrc, sort = TRUE)
nrc_word_counts
nrc_word_counts %>%
filter(n > 10000) %>%
mutate(n = ifelse(nrc == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = nrc)) +  geom_col() +  coord_flip() +  labs(y = "Contribution to sentiment")
nrc_word_counts %>%
filter(!is.na(nrc)) %>%
ggplot(aes(word, n, fill = nrc)) +
geom_col() +
coord_flip() +
labs(y = "Contribution to sentiment")
nrc_word_counts %>%
filter(!is.na(nrc)) %>%
ggplot(aes(word, n, fill = nrc)) +
labs(y = "Contribution to sentiment")
nrc_word_counts %>%
filter(!is.na(nrc)) %>%
ggplot(.,aes(x = nrc)) +
geom_bar()
review_sentences <- df %>%
unnest_tokens(output = sentence, input = text, token = "sentences")
review_sentences
library(sentimentr)
sentiment('I am not very happy. He is very happy')
devtools::install_github("cloudyr/RoogleVision")
knitr::opts_chunk$set(echo = TRUE)
tokens <- rc_rel %>%
unnest_tokens(output = word, input = comment)
devtools::install_github("cloudyr/googleComputeEngineR")
r <- reddit_urls(subreddit = "Coronavirus", page_threshold = 10)
library(RedditExtractoR)
library(tidyverse)
library(wordcloud)
library(tidytext)
library(recipes)
library(skimr)
library(h2o)
library(readr)
library(httr)
library(RCurl)
r <- reddit_urls(subreddit = "Coronavirus", page_threshold = 10)
rc <- reddit_content(r$URL)
rc_rel <- rc[c("comment", "comment_score", "post_score")]
rc_rel <- mutate(rc_rel, id = rownames(rc_rel))
tokens <- rc_rel %>%
unnest_tokens(output = word, input = comment)
sw = get_stopwords()
cleaned_tokens <- tokens %>%
filter(!word %in% sw$word)
nums <- cleaned_tokens %>%
filter(str_detect(word, "^[0-9]|http[s]?|.com$")) %>%
select(word) %>% unique()
cleaned_tokens <- cleaned_tokens %>%
filter(!word %in% nums$word)
rare <- cleaned_tokens %>%
count(word) %>%
filter(n < 10) %>%
select(word) %>%
unique()
cleaned_tokens <- cleaned_tokens %>%
filter(!word %in% rare$word)
pal <- brewer.pal(8,"Dark2")
cleaned_tokens %>%
count(word) %>%
with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = pal))
sent_reviews = cleaned_tokens %>%
left_join(get_sentiments("nrc")) %>%
rename(nrc = sentiment) %>%
left_join(get_sentiments("bing")) %>%
rename(bing = sentiment) %>%
left_join(get_sentiments("afinn")) %>%
rename(afinn = value)
sent_reviews <- sent_reviews %>%
mutate(bing = ifelse(word == "corona", "negative", bing)) %>%
mutate(nrc = ifelse(word == "corona", "negative", nrc))
bing_word_counts <- sent_reviews %>%
filter(!is.na(bing)) %>%
count(word, bing, sort = TRUE)
bing_word_counts %>%
filter(n > 250) %>%
mutate(n = ifelse(bing == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = bing)) +  geom_col() +
coord_flip() +
labs(y = "Contribution to sentiment")
nrc_word_counts <- sent_reviews %>%
filter(!is.na(nrc)) %>%
count(word, nrc, sort = TRUE)
nrc_word_counts %>%
filter(n > 250) %>%
mutate(n = ifelse(nrc %in% c("negative", "fear", "anger", "sadness", "disgust"), -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = nrc)) +  geom_col() +
coord_flip() +
labs(y = "Contribution to sentiment")
sent_reviews %>%
filter(!is.na(nrc)) %>%
group_by(nrc) %>%
summarise(count = n()) %>%
ggplot(., aes(x = reorder(nrc, -count), y = count)) +
geom_bar(stat = "identity") +
xlab("NRC") +
labs(title = "Distribution of sentiments")
df_corona <- rc %>% filter(str_detect(title,"corona") |
str_detect(title,"Corona") |
str_detect(title,"CORONA") |
str_detect(title,"2019-nCOV") |
str_detect(title,"COVID-19") |
str_detect(title,"Covid-19") |
str_detect(title,"covid-19")) %>%
select(comm_date) %>%
group_by(comm_date) %>%
count(comm_date)
#change the file Name of Corona
c_name <- c("Date","Count_Corona")
colnames(df_corona) <- c_name
# Aggregate World
df_world <- rc %>%
select(comm_date) %>%
group_by(comm_date) %>%
count(comm_date)
#change the file Name of World
w_name <- c("Date","Count_World")
colnames(df_world) <- w_name
df_export <- inner_join(df_world,df_corona, by ="Date")
if (file.exists("corona_over_time.csv") == TRUE){
write.table(df_export,"corona_over_time.csv",
append = TRUE, col.names = FALSE, sep =",", row.names=FALSE)
} else {
write.table(df_export,"corona_over_time.csv",
append = FALSE, col.names = TRUE, sep =",", row.names=FALSE)
}
urlfile="https://raw.githubusercontent.com/hnguyen154/RedditBigDataProject/master/corona_over_time.csv"
#Get the csv from Github
x <- getURL(urlfile)
df_corona_plot <- read.csv(text = x)
#Untidy data for visualization
df_plot <- df_corona_plot %>%
gather('Count_Corona', 'Count_World', key = "Categories", value = "Total_Comments")
#Plot the Corona Concern
ggplot(df_plot, aes(fill=Categories, y=Total_Comments, x=Date)) +
geom_bar(position="dodge", stat="identity") +geom_line(aes(x=Date, y=Total_Comments, group=Categories))+
geom_point()
sent_reviews <- sent_reviews %>%
mutate(id = as.integer(id))
df0 <- sent_reviews %>%
select("id", "comment_score", "post_score") %>%
unique()
nrcs <- sent_reviews %>%
select("nrc") %>%
unique() %>%
na.omit() %>%
as.list()
for (nrcx in nrcs$nrc) {
df1 <- sent_reviews %>%
select("id", "nrc") %>%
filter(nrc == nrcx) %>%
group_by(id) %>%
summarise(num = n()) %>%
arrange(id)
colnames(df1)[2] <- paste0("nrc_", nrcx)
df0 <- full_join(df0, df1, by = "id")
}
x_train_tbl <- df0 %>% select(-c(comment_score, id))
y_train_tbl <- df0 %>% select(comment_score)
x_train_tbl_skim = partition(skim(x_train_tbl))
rec_obj <- recipe(~ ., data = x_train_tbl) %>%
step_meanimpute(all_numeric()) %>%
prep(training = x_train_tbl)
x_train_processed_tbl <- bake(rec_obj, x_train_tbl)
h2o.init(nthreads = -1)
h2o.removeAll()
data_h2o <- as.h2o(
bind_cols(y_train_tbl, x_train_processed_tbl),
destination_frame= "train.hex"
)
splits <- h2o.splitFrame(data = data_h2o,
ratios = c(0.7, 0.15),
seed = 1234)
train_h2o <- splits[[1]]
valid_h2o <- splits[[2]]
test_h2o  <- splits[[3]]
y <- "comment_score"
x <- setdiff(names(train_h2o), y)
hyper_prms <- list(
ntrees = c(45, 50, 55),
learn_rate = c(0.1, 0.2),
max_depth = c(20, 25)
)
grid_gbm <- h2o.grid(
grid_id="grid_gbm",
algorithm = "gbm",
training_frame = train_h2o,
validation_frame = valid_h2o,
x = x,
y = y,
sample_rate = 0.7,
col_sample_rate = 0.7,
stopping_rounds = 2,
stopping_metric = "RMSE",
stopping_tolerance = 0.0001,
score_each_iteration = T,
model_id = "gbm_covType3",
balance_classes = T,
seed = 5000,
hyper_params = hyper_prms
)
grid <- h2o.getGrid("grid_gbm", sort_by = "rmse", decreasing = FALSE)
dl_grid_best_model <- h2o.getModel(grid@summary_table$model_ids[1])
dl_grid_best_model
rm(list = ls())
setwd("/Users/haonguyen/Documents/GitHub/BigDataAnalyticFinalProject")
# install.packages("kknn")
library(kknn)
# install.packages("class")
library(class)
library(e1071)
library(lattice)
library(ggplot2)
library(caret)
# install.packages("DMwR")
library(DMwR)    # For KNN
# install.packages("ggvis")
library(ggvis)
library(dplyr)
library(randomForest)
##---------------------------------IMPORT DATA---------------------------------
#Original dataset has splitted by 70% training and 30% testing
X_train<-read.table("./UCI HAR Dataset/train/X_train.txt")
y_train<-read.table("./UCI HAR Dataset/train/y_train.txt")
X_test<-read.table("./UCI HAR Dataset/test/X_test.txt")
y_test<-read.table("./UCI HAR Dataset/test/y_test.txt")
y_train[,'V1'] = as.factor(y_train[,'V1'])
y_test[,'V1'] = as.factor(y_test[,'V1'])
#To understand what is each column associated with what.
features<-read.table("./UCI HAR Dataset/features.txt")
sum(is.na(X_train))
sum(is.na(y_train))
sum(is.na(X_test))
sum(is.na(y_test))
class_train = y_train[,1]
class_test = y_test[,1]
## A 5-nearest neighbors model with no normalization
knn5_pred <- knn(train = X_train, test = X_test, cl = class_train, k=5)
#NROW(knn5_pred) # compare size with y_test
##Evaluation KNN_5
table(knn5_pred , class_test)
confusionMatrix(table(knn5_pred , class_test))
data.rf=randomForest(class_train ~ ., data=X_train, ntree=100, mtry=2, importance=TRUE)
data.rf #Confustion matrix
varImpPlot(data.rf)
X_test$pred_rf<-predict(object = data.rf,X_test)
X_test$pred_rf.prob<-predict(object = data.rf,X_test,type="prob")
confusionMatrix(class_test,X_test$pred_rf)
up.fit.RF <- upliftRF(class_train ~ ., data=X_train, mtry = 3, ntree = 100, split_method = "KL",
minsplit = 200, verbose = TRUE)
pred <- predict(up.fit.RF, newdata = X_test)
# install.packages("uplift")
library(uplift)
up.fit.RF <- upliftRF(class_train ~ ., data=X_train, mtry = 3, ntree = 100, split_method = "KL",
minsplit = 200, verbose = TRUE)
pred <- predict(up.fit.RF, newdata = X_test)
